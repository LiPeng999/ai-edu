## 解决软间隔问题

### 实例学习一

我们先用图 5.7.1 的问题来学习一下 C 值对分类结果的影响。首先定义数据：

```Python
X = np.array([[0,3],[1,1],[1,2],[2,1],[3,0],[1,3],[1,4],[2,3],[3,1],[3,2],[3,3],[4,1]])
Y = np.array([-1,-1,-1,-1,-1,-1,1,1,1,1,1,1])
```

如果绘制到图中观察，可以发现正负类样本实际上是成对称分布的，中间有一个很宽的分类间隔，但是由于负类样本点 5 和正类样本点 8 的存在，侵占了原有的分类间隔，所以必须重新计算。

我们分别设置 C 的值为 10、1、0.1，看看分类效果如何。

```Python
fig = plt.figure()
ax = fig.add_subplot(131)
C = 10
svc(ax, C, X, Y)
ax = fig.add_subplot(132)
C = 1
svc(ax, C, X, Y)
ax = fig.add_subplot(133)
C = 0.1
svc(ax, C, X, Y)
plt.show()
```

上述代码中的 svc 函数实际上是调用 sklearn 的 SVM 模块中的 SVC 方法来实现的：

```Python
from sklearn.svm import SVC

model = SVC(C, kernel='linear')
model.fit(X,Y)
```

绘制样本点：

```Python
    for i in range(Y.shape[0]):
        if (Y[i] == 1):
            # 正类样本
            ax.scatter(X[i,0], X[i,1], marker='x', color='r')
        else:
            # 负类样本
            ax.scatter(X[i,0], X[i,1], marker='.', color='b', s=200)
        # 样本编号
        ax.text(X[i,0]+0.1, X[i,1]+0.1, str(i))
```

同时，把分界线及分类间隔绘制出来：

- 分界线方程
  
  $w_1 x_1 + w_2 x_2 + b = 0$，转换成 $y=ax+b$ 的形式，即：$x_2 = -\frac{w_1}{w_2}x_1-\frac{b}{w_2}$，

- 分类间隔边界的方程

  $w_1 x_1 + w_2 x_2 + b = \plusmn 1$，转换成 $x_2 = -\frac{w_1}{w_2}x_1-\frac{b}{w_2} \plusmn \frac{1}{w_2}$

在下面的代码中，$w$ 矢量的下标是从 0 开始的，所以 $w_1 = w[0]，w_2=w[1]$：


```Python
    w = model.coef_[0]
    a = -w[0]/w[1]
    b = model.intercept_
    x = np.linspace(0,5,10)
    # 分界线
    y0 = a * x + -b/w[1]
    # 上边界
    y1 = a * x + -b/w[1] + 1/w[1]
    # 下边界
    y2 = a * x + -b/w[1] - 1/w[1]
    ax.plot(x,y0)
    ax.plot(x,y1,linestyle='--')
    ax.plot(x,y2,linestyle='--')
```

最后得到图 5.8.1。

<img src="./images/5-8-1.png" />

<center>图 5.8.1 </center>

||C=10|C=1|C=0.1|
|--|--|--|--|
|W|[3,2]|[1,1]|[0.66,0.33]|
|b|-10|-4|-2|
|支持向量|4,5; 8|4,5; 7,8|0,1,2,3,4,5; 6,7,8,9,10,11|

- C=10

  C=10 时，要求尽量分类两类样本，分类间隔会变窄。支持向量只有 3 个，序号为4、5、8的三个样本。其实 6 也可以是支持向量，只不过有4、5、8三个样本点已经能够完全决定分类间隔的形状了。

- C=1

  C=1 时，属于比较中庸的选择，结果也比较合理。本来 5 号和 8 号样本点应该算是噪音，所以这种选择很合理地把 0、2、3、4 放在了分类间隔下界上，同时把 6、7、9、11 放在了分类间隔上届上，把 5、8 放在了分界线上。

- C=0.1

  C=0.1 时，要求分类间隔尽量宽，所以两类样本点都处于分类间隔之内，即都是支持向量。可以看到是三种情况中最宽的间隔。


### 实例学习二

所示的三条分界线都不能很好地把两类样本完全分开：

- 左子图的分界线 L1，有一个负类样本（圆黑点）混在了右侧的正类样本（三角形）之间；
- 中子图的分界线 L2，各有一个正负类样本在分界线错误的一边，而且各有一个正负类样本骑在了分界线上；
- 右子图的分界线 L3，照顾了远处的负类样本，但是两个正类样本被错误分类。









下面我们看一个真实的例子。有正类样本 6 个，负类样本 5 个，值如下：

```
X = np.array([[3,0],[2,4],[3,4],[3,3],[3,2],[3,1],[1,1],[2,2],[2,1],[2,0],[4,1]])
Y = np.array([1,1,1,1,1,1,-1,-1,-1,-1,-1])
```

使用 sklearn 自带的 SVC 模块来进行分类，设置参数C=1：

```python
from sklearn.svm import SVC

model = SVC(C=1, kernel='linear')
model.fit(X,Y)
```

执行 fit() 方法后，得到分类结果如表 5-7-1 所示。

|样本|0|1|2|3|4|5|6|7|8|9|10|
|--|--|--|--|--|--|--|--|--|--|--|--|
|$x_1$|3|2|3|3|3|3|1|2|2|2|4|
|$x_2$|0|4|4|3|2|1|1|2|1|0|1|
|$y$|1|1|1|1|1|1|-1|-1|-1|-1|-1|
|距离|0|1|2|1.5|1|0.5|-1.5|0|-0.5|-1|1.5|
|支持向量|是|是|不|不|是|是|不|是|是|是|是|
|$\alpha * y$|1|0.375|0|0|1|1|0|-1|-1|-0.375|-1|
|$\alpha$|1|0.375|0|0|1|1|0|1|1|0.375|1|





形成图 5.7.3 的左子图。

<img src="./images/5-7-3.png" />

<center>图 5.7.3 软间隔中的各种典型样本点</center>





|样本|0|1|2|3|4|5|6|7|8|9|10|
|--|--|--|--|--|--|--|--|--|--|--|--|
|$x_1$|3|2|3|3|3|3|1|2|2|2|4|
|$x_2$|0|4|4|3|2|1|1|2|1|0|1|
|$y$|1|1|1|1|1|1|-1|-1|-1|-1|-1|
|距离|-0.7|1|1.3|0.8|0.3|-0.2|-0.8|0|-0.5|-1|1|
|支持向量|是|是|不|是|是|是|是|是|是|是|是|
|$\alpha * y$|0.1|0.1|0|0.1|0.1|0.1|-0.1|-0.1|-0.1|-0.1|-0.1|
|$\alpha$|0.1|0.1|0|0.1|0.1|0.1|0.1|0.1|0.1|0.1|0.1|
