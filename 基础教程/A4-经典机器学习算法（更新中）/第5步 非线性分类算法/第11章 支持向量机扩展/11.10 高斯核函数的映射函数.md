
## 11.10 高斯核函数的映射函数

### 11.10.1 实际的映射函数

上一节中，我们推导了连续高斯核函数的内积形式，这一节我们研究高斯核函数内积如何应用到离散变量。

我们仍以异或问题为例，讲解映射函数的特征矩阵生成方式。

1. 首先在内定义一个区域，这个区域比样本空间略微大一些。比如在图 11.10.1 中，样本空间的区域是 $x \in [0,1],y \in [0,1]$，则区域可以定义为 $x \in [-0.5,1.5],y \in [-0.5,1.5]$，这样就可以涵盖所有 4 个样本点。

<img src="./images/11-10-1.png" />

<center>图 11.10.1 异或问题特征矩阵的生成方式</center>

2. 由于是离散问题，所以在上述区域内定义一个 5x5 的网格，一共有 25 个采样点，我们把这些采样点叫做地标（landmark），每个地标都有自己的坐标，如：

$$L_i=[l_{i,1},l_{i,2}], \quad i=1,...,25$$

其中 $L_0=[-0.5,-0.5],L_{25}=[1.5,1.5]$。

代码如下：

```python
# 采样范围和密度
scope = [-0.5,1.5,5, -0.5,1.5,5]

def create_landmark(scope):
    #scope = [-0.5,1.5,5, -0.5,1.5,5]
    x1 = np.linspace(scope[0], scope[1], scope[2])
    # 从1到-0.5，相当于y值从上向下数，便于和图像吻合
    x2 = np.linspace(scope[4], scope[3], scope[5])

    landmark = np.zeros((scope[2]*scope[5], 2))
    for i in range(scope[2]):
        for j in range(scope[5]):
            landmark[i*scope[2]+j,0] = x1[j]
            landmark[i*scope[2]+j,1] = x2[i]

    return landmark
```

其中，-0.5 表示起始位置，1.5 表示终止位置，5 表示采样密度，也就是每行每列都有 5 个采样点，一共 25 个。

3. 对于每个样本 $x_i$，定义以该样本坐标为中心的高斯函数：
   
$$
f(x_i) = e^{-\gamma \big [(X1-x_{i,1})^2+(X2-x_{i,2})^2 \big ]} =e^{-\gamma ||X-x_{i}||^2}
\tag{11.10.1}
$$

比如，对于样本点 $x_2(0,1)$，有 $f(x_3)=e^{-\gamma \big[(X1-0)^2+(X2-1)^2\big]}$。

4. 在每个地标上，把所有的 $L_i=[l_{i,1},l_{i,2}]$ 代入式 11.10.1 替换 $X1,X2$，计算出 25 个值，就是每个样本的 25 个特征值。

```python
# 映射特征矩阵
# X - 样本数据
# L - 地标 Landmark，在此例中就是样本数据
# gamma - 形状参数
def Feature_matrix(X, L, gamma):
    n = X.shape[0]  # 样本数量
    m = L.shape[0]  # 特征数量
    X_feature = np.zeros(shape=(n,m))
    for i in range(n):
        for j in range(m):
            # 计算每个样本点在地标上的高斯函数值，式 11.10.1 
            X_feature[i,j] = np.exp(-gamma * np.linalg.norm(X[i] - L[j])**2)

    return X_feature
```

上述代码计算了 4 个样本的 25 个特征值，打印输出如下：

```
Feature Matrix=
[[0.082 0.105 0.082 0.039 0.011 0.287 0.368 0.287 0.135 0.039 0.607 0.779
  0.607 0.287 0.082 0.779 1.    0.779 0.368 0.105 0.607 0.779 0.607 0.287
  0.082]
 [0.082 0.287 0.607 0.779 0.607 0.105 0.368 0.779 1.    0.779 0.082 0.287
  0.607 0.779 0.607 0.039 0.135 0.287 0.368 0.287 0.011 0.039 0.082 0.105
  0.082]
 [0.607 0.779 0.607 0.287 0.082 0.779 1.    0.779 0.368 0.105 0.607 0.779
  0.607 0.287 0.082 0.287 0.368 0.287 0.135 0.039 0.082 0.105 0.082 0.039
  0.011]
 [0.011 0.039 0.082 0.105 0.082 0.039 0.135 0.287 0.368 0.287 0.082 0.287
  0.607 0.779 0.607 0.105 0.368 0.779 1.    0.779 0.082 0.287 0.607 0.779
  0.607]]
```
其中，把 2 号样本的特征值变成 5x5 的矩阵如下：
```
2 号样本的特征值矩阵：
[[0.607 0.779 0.607 0.287 0.082]
 [0.779 1.    0.779 0.368 0.105]
 [0.607 0.779 0.607 0.287 0.082]
 [0.287 0.368 0.287 0.135 0.039]
 [0.082 0.105 0.082 0.039 0.011]]
```

上述的特征矩阵值的具体含义，实际上就是高斯函数在离散值上的具体数值。

<img src="./images/11-10-2.png" />

<center>图 11.10.2 </center>

图 11.10.2 中，以 2 号样本为例：
- 第 2 行第 2 列的值为 1，即该地标点与样本点重合；
- 左上角的地标的高斯函数值为 0.607，小于样本点 2 正上方的地标的高斯函数值 0.779；
- 由于高斯函数是对称的，所以样本点上下左右的地标的值都是 0.779，四个斜角的地标的值都是 0.607；
- 距离最远的右下角的地标值最小，为 0.011。

所以，这个特征矩阵描述了每个样本点在此区域内的“势力范围”，距离越近，影响越大，附近区域的分类越趋向于该样本点的类别。


5. 当在 SVM 算法中需要计算 $\langle x_i,x_j \rangle$ 的内积时，把两行数据按位相乘再相加即可。比如 $\langle x_1,x_3 \rangle$ 的内积就是 Feature Matrix 中第一行和第三行数据的内积。

### 11.10.2 验证线性可分性

在上面的第 4 步中，已经生成好了特征矩阵 Feature Matrix，命名为 X_feature，把它作为线性 SVM 的特征数据，训练出一个模型，代码如下：

```python
    C = 1
    model = linear_svc(X_feature, Y, C)
```
得到的分类结果 score=1.0，表明样本分类正确，特征数据建立成功！

用可视化方法观察一下分类效果，代码如下：

```python
    # 可视化结果
    scope3 = [-0.5,1.5,50, -0.5,1.5,50]
    X1,X2,y_pred,prob = prediction(model, gamma, L, scope3)
    print(y_pred)
    show_result(X1, X2, y_pred, X_raw, Y, prob, scope3)
```
y_pred 是做完符号函数后的结果，所以只有 1 和 -1：
```
[[-1 -1 -1 ...  1  1  1]
 [-1 -1 -1 ...  1  1  1]
 [-1 -1 -1 ...  1  1  1]
 ...
 [ 1  1  1 ... -1 -1 -1]
 [ 1  1  1 ... -1 -1 -1]
 [ 1  1  1 ... -1 -1 -1]]
```

prob 是 model.decision_function(x) 返回的，x 是被预测的样本，prob 是各个样本距离分界线的距离，也可以理解为是一种分类概率，大于 0 的是正类，值越大距离分界线越远。

绘图结果如图 11.10.3 所示。左子图是按区域分类，非正即负；右子图显示了一种概率，颜色越暖（红色）越靠近正类。

<img src="./images/11-10-3.png" />

<center>图 11.10.3 分类结果</center>

同时打印出权重：
```
权重5x5:
 [[-0.453 -0.426  0.     0.426  0.453]
  [-0.426 -0.4    0.     0.4    0.426]
  [ 0.     0.     0.     0.     0.   ]
  [ 0.426  0.4    0.    -0.4   -0.426]
  [ 0.453  0.426  0.    -0.426 -0.453]]
```

我们把权重值同样 reshape 成 5x5 的矩阵，可以发现处于十字中心线的几个地标上的权重值为 0，对应到图 11-10-2 上，就是绿色的点位的地标的权重值为 0，也就是说这个 9 个特征是无效的。

<img src="./images/11-10-4.png" />
<center>图 11.10.4 线性分类后得到的权重</center>

这是因为原始样本点是中心对称分布的，所以在其中间地带的特征有二义性，不能被使用于识别类别。


### 思考和练习

